# Goal : Introduction to text mining using R
# Reference : R을 활용한 텍스트 마이닝 입문 (저자:서진수)


# 0. 현재 많이 사용되는 텍스트 분석 기법들 ------------------------------------------------

# 1) 핵심 키워드 추출 및 분석: 단어의 출현 빈도 카운트 및 워드클라우드
# 단어들이 발생한 빈도 후 워드클라우드로 시각화하여 핵심키워드를 도출
# 2) 단어간의 관계 분석: Co-occurrence Matrix
# 문서 내에서 동시에 빈번하게 출현하는 단어를 파악하여 단어간의 상관관계 계산
# 3) 감성 분석(감정 분석) : 감성사전과 단어 비교 대조하여 감정 점수산출
# 긍정, 부정감성사전이 만들어져 있다면 사전과 대조하여 긍정단어와 부정단어의 빈도를 추출한 후 긍정단어가 나타나면 +1, 부정단어가 나타나면 -1의 점수를 부여해 문서의 감성을 수치화함
# 4) 토픽 분석: 문서 내에 어떤 주제나 이슈가 존재하는지 파악할 때 사용
# - LSA: 분절된 단어들에 벡터값을 부여하고 차원축소를 하여 축소된 차원에서 근접한 단어들을 주제로 묶음
# - LDA: 확률을 바탕으로 단어가 특정 주제에 존재할 확률과 문서에 특정 주제가 존재할 확률을 결합확률로 추정하여 토픽추출
# 5) 맥락에 따른 단어 및 문서 분석: Bag of words, word2vec, doc2vec
# - Bag of words: 텍스트 분석의 단위를 개별단어가 아닌 단어들의 모음으로 구분하는 방법(각 단어는 문서의 특징이 됨)
# - word to vec: 문장(단어들의 모음)자체를 뉴럴넷에 입력하여 뉴럴넷이 문장 내에 있는 단어들에 벡터값을 부여하게 함


# 1. 먼저 알아야 할 텍스트 처리 기본 사항들 -----------------------------------------------

# 1.1 R에서 영어를 처리하는 일반적인 방법 ------------------------------------------------

txt1 <- "Start R programming with R-LOVE book."
txt1
# [1] "Start R programming with R-LOVE book."

strsplit(txt1," ")
# [[1]]
# [1] "Start"       "R"           "programming" "with"        "R-LOVE"      "book."     


# 1.2 R에서 한글을 처리하는 방법 -----------------------------------------------------

# 1.2.1 공백을 기준으로 단어 분리하기 --------------------------------------------------

# install.packages(“KoNLP”)
library(KoNLP)
# install.packages(“stringr”)
library(stringr)

txt2 <- "R라뷰 책으로 R 프로그래밍을 시작하세요~!"
txt2
# [1] "R라뷰 책으로 R 프로그래밍을 시작하세요~!"

strsplit(txt2," ")
# [[1]]
# [1] "R라뷰" "책으로" "R" "프로그래밍을" "시작하세요~!"

extractNoun(txt2)
# [1] "R라뷰" "책" "R" "프로그래밍" "시작"


# 1.2.2 품사까지 자세하게 출력하기 - SimplePos09( ) / SimplePos22( )함수 사용 -------------

SimplePos09(txt2)
# $`R라뷰`
# [1] "R라뷰/N"
# 
# $책으로
# [1] "책/N+으로/J"
# 
# $R
# [1] "R/F"
# 
# $프로그래밍을
# [1] "프로그래밍/N+을/J"
# 
# $`시작하세요~`
# [1] "시작/N+하/X+세/E+요/J+~/S"
# 
# $`!`
# [1] "!/S"


# 1.2.3 원하는 품사만 골라내기 -------------------------------------------------------

txt3 <- "우리 모두 R라뷰 책으로 정말 재미있게 공부해요"
txt4 <- SimplePos09(txt3)
txt4
# $우리
# [1] "우리/N"
# 
# $모두
# [1] "모두/M"
# 
# $R라뷰
# [1] "R라뷰/N"
# 
# $책으로
# [1] "책/N+으로/J"
# 
# $정말
# [1] "정말/M"
# 
# $재미있게
# [1] "재미있/P+게/E"
# 
# $공부해
# [1] "공부해/N"
# 
# $요
# [1] "요/M"

txt_n <- str_match(txt4,'([A-Z가-힣]+)/N') # 명사 확인하기
txt_n
# [,1]       [,2]    
# [1,] "우리/N"   "우리"  
# [2,] NA         NA      
# [3,] "R라뷰/N"  "R라뷰" 
# [4,] "책/N"     "책"    
# [5,] NA         NA      
# [6,] NA         NA      
# [7,] "공부해/N" "공부해"
# [8,] NA         NA      

txt_p <- str_match(txt4,'([A-Z가-힣]+)/P')
txt_p
# [,1]       [,2]    
# [1,] NA         NA      
# [2,] NA         NA      
# [3,] NA         NA      
# [4,] NA         NA      
# [5,] NA         NA      
# [6,] "재미있/P" "재미있"
# [7,] NA         NA      
# [8,] NA         NA 

txt_np <- str_match(txt4,'([A-Z가-힣]+)/[NP]')
txt_np
# [,1]       [,2]    
# [1,] "우리/N"   "우리"  
# [2,] NA         NA      
# [3,] "R라뷰/N"  "R라뷰" 
# [4,] "책/N"     "책"    
# [5,] NA         NA      
# [6,] "재미있/P" "재미있"
# [7,] "공부해/N" "공부해"
# [8,] NA         NA


# 3. 한글 텍스트 분석하기 ----------------------------------------------------------

# 3.1 KoNLP( ) 패키지 기본 기능 익히기 ----------------------------------------------

# 이 패키지는 R 에서 한국어를 보다 쉽게 사용하기 위해 "전희원" 님께서 만드신 아주 유용한 패키지입니다. 
# 한국어 관련 작업에서는 대부분 다 사용이 됩니다. 이 패키지에 관련된 자세한 내용은 구글에서 
# "KoNLP in r " 로 검색하시면 자세한 정보를 확인할 수 있습니다. 이 패키지에는 다양한 함수가 존재하는데 
# 전체 내용을 다루기엔 지면 관계상 분량이 너무 많으므로 주로 사용되고 꼭 알아야 하는 함수에 대해 살펴보겠습니다.

# 3.1.1 extractNoun ( ) 함수 활용하기 - 한글의 명사 추출 함수 ----------------------------

# 이 함수는 한글을 입력 받아서 명사를 추출 해 주는 역할을 합니다. 함수 이름 쓸 때 대소문자 특히 조심하세요. 
# 이 함수는 Hannanum analyzer (한나눔 분석기)를 사용한다고 패키지의 저자가 밝히고 있습니다. 
# 한나눔에 대해서 더 자세한 정보를 원하시는 분들은 (http://kldp.net/projects/hannanum) 를 참고하시기 바랍니다.

# 문법 : extractNoun(분석할 문장이나 변수)
# txt1 <- readLines("좋아하는과일.txt")
txt1 <- c("나는 사과와 바나나를 좋아합니다.",
  "나는 바나나 바나나 바나나 바나나 바나나가 최고 좋아요!",
  "나는 복숭아와 사과를 좋아합니다.",
  "나는 복숭아와 사과를 좋아합니다.",
  "나는 사과와 포도를 좋아합니다.",
  "나는 파인애플과 복숭아를 좋아합니다."
)
txt1
# [1] "나는 사과와 바나나를 좋아합니다."                       "나는 바나나 바나나 바나나 바나나 바나나가 최고 좋아요!"
# [3] "나는 복숭아와 사과를 좋아합니다."                       "나는 복숭아와 사과를 좋아합니다."                      
# [5] "나는 사과와 포도를 좋아합니다."                         "나는 파인애플과 복숭아를 좋아합니다."    

txt2 <- extractNoun(txt1)
txt2
# [[1]]
# [1] "나"     "사과"   "바나나"
# 
# [[2]]
# [1] "나"     "바나나" "바나나" "바나나" "바나나" "바나나" "최고"  
# 
# [[3]]
# [1] "나"     "복숭아" "사과"  
# 
# [[4]]
# [1] "나"     "복숭아" "사과"  
# 
# [[5]]
# [1] "나"   "사과" "포도"
# 
# [[6]]
# [1] "나"       "파인애플" "복숭아"

# 명사를 잘 찾아 주는 것이 보이죠? 그런데 이 함수는 공백을 기준으로 단어를 판단합니다.
# 아래 예문처럼 띄어쓰기를 잘 못하면 명사를 잘 못 찾게 됩니다.

v2 <- ("봄이지나 면여름이고 여름이지나면가을 입니다")
extractNoun(v2)
# [1] "면여름이고" "여름"  

# 3.1.2 사전 활용하기 -----------------------------------------------------------

# 위에서 살펴 본 바와 같이 extractNoun( ) 함수는 공백 단위로 단어를 만들고 명사를 골라 냅니다. 
# 그런데 이 함수는 과연 그 단어가 명사인지 아닌지는 어떻게 알까요?
# 바로 내부적으로 가지고 있는 사전과 비교를 하기 때문에 알 수 있는 것입니다.
# 먼저 사전 파일의 위치부터 확인해 보겠습니다.

# 사전 파일의 경로는 아래와 같이 확인할 수 있습니다.
.libPaths()
# [1] "C:/Program Files/R/R-3.5.2/library"

# 위 두 경로는 주요 패키지들이 설치되는 장소입니다.
# 그 안에 KoNLP_dic 라는 폴더가 있는데 그 안에 가면 backup 과 current 폴더가 있고
# current 라는 폴더에 가면 dic_user.txt 라는 파일이 사전 파일입니다.

useSejongDic( )
# Backup was just finished!
# 370957 words dictionary was built.

# 위와 같이 사전을 사용하라고 설정 한 후 한글 텍스트로 테스트 해 보겠습니다.
txt_5 <- "우리는 유관순 의사와 안중근 의사가 독립투사임을 반드시 기억합시다"
extractNoun(txt_5)
# [1] "우리"     "유관"     "순"       "의사"     "안중"     "근"       "의사"     "독립투사" "기억"     "합"

# 결과가 약간 생각과 다르게 나오죠?
# 유관순, 안중근 이런 단어들이 분리되어서 나오는 이유는 사전에 등록이 안되었기 때문입니다.
# 위의 예에서 알 수 있듯이 정확한 한글 분석을 하기 위해서는 분석 작업을 하기 전에 해당 용어들을 사전에 아래와 같이 추가를 해야 합니다.

mergeUserDic(data.frame(c('유관순','안중근'),c('ncn')))
# 2 words were added to dic_user.txt.

extractNoun(txt_5)
# [1] "우리"     "유관순"   "의사"     "안중근"   "의사"     "독립투사" "기억"     "합"      

# 참고로 만약 사전에 추가해야 할 내용이 많을 경우는 추가할 단어를 파일에 저장해 둔 후 아래와 같이 자동으로 불러서 추가할 수 있습니다.
# mergeUserDic(data.frame(readLines("mergefile.txt"), "ncn"))

# 3.1.3 중복되는 값 제거하기 -------------------------------------------------------

txt1 <- c("나는 사과와 바나나를 좋아합니다.",
          "나는 바나나 바나나 바나나 바나나 바나나가 최고 좋아요!",
          "나는 복숭아와 사과를 좋아합니다.",
          "나는 복숭아와 사과를 좋아합니다.",
          "나는 사과와 포도를 좋아합니다.",
          "나는 파인애플과 복숭아를 좋아합니다."
)
txt1
# [1] "나는 사과와 바나나를 좋아합니다."                       "나는 바나나 바나나 바나나 바나나 바나나가 최고 좋아요!"
# [3] "나는 복숭아와 사과를 좋아합니다."                       "나는 복숭아와 사과를 좋아합니다."                      
# [5] "나는 사과와 포도를 좋아합니다."                         "나는 파인애플과 복숭아를 좋아합니다."    

# 위 예에서 2번 줄을 보면 1 사람이 바나나를 5번 언급했습니다.
# 이 경우에는 5번으로 카운트 되면 안되고 1번으로 카운트 되도록 코드가 작성되어야 더 정확한 결과가 나올 것입니다.
# 그리고 3번줄과 4번줄은 완전 동일한 문장입니다. 즉 1 명이 2번 작성을 했을 확률이 아주 높기에 제거를 해야 합니다.
# (이런 대표적인 예가 트위터 데이터의 리트윗 제거 입니다)
# 이번에는 이렇게 중복인 데이터가 있을 경우 어떻게 처리를 해야 하는지 간단하게 살펴 보겠습니다.

txt1 <- c("나는 사과와 바나나를 좋아합니다.",
          "나는 바나나 바나나 바나나 바나나 바나나가 최고 좋아요!",
          "나는 복숭아와 사과를 좋아합니다.",
          "나는 복숭아와 사과를 좋아합니다.",
          "나는 사과와 포도를 좋아합니다.",
          "나는 파인애플과 복숭아를 좋아합니다."
)
txt1

txt2 <- Map(extractNoun,txt1)
txt2
# $`나는 사과와 바나나를 좋아합니다.`
# [1] "나"     "사과"   "바나나"
# 
# $`나는 바나나 바나나 바나나 바나나 바나나가 최고 좋아요!`
# [1] "나"     "바나나" "바나나" "바나나" "바나나" "바나나" "최고"  
# 
# $`나는 복숭아와 사과를 좋아합니다.`
# [1] "나"     "복숭아" "사과"  
# 
# $`나는 복숭아와 사과를 좋아합니다.`
# [1] "나"     "복숭아" "사과"  
# 
# $`나는 사과와 포도를 좋아합니다.`
# [1] "나"   "사과" "포도"
# 
# $`나는 파인애플과 복숭아를 좋아합니다.`
# [1] "나"       "파인애플" "복숭아"  

txt3 <- unique(txt2) # 중복되는 리스트를 제거합니다.
txt3
# [[1]]
# [1] "나"     "사과"   "바나나"
# 
# [[2]]
# [1] "나"     "바나나" "바나나" "바나나" "바나나" "바나나" "최고"  
# 
# [[3]]
# [1] "나"     "복숭아" "사과"  
# 
# [[4]]
# [1] "나"   "사과" "포도"
# 
# [[5]]
# [1] "나"       "파인애플" "복숭아"  

txt4 <- lapply(txt3,unique) # 각 리스트 안에서 중복되는 단어를 제거합니다
txt4
# [[1]]
# [1] "나"     "사과"   "바나나"
# 
# [[2]]
# [1] "나"     "바나나" "최고"  # 이 줄이 중복이 제거되었죠?
# 
# [[3]]
# [1] "나"     "복숭아" "사과"  
# 
# [[4]]
# [1] "나"   "사과" "포도"
# 
# [[5]]
# [1] "나"       "파인애플" "복숭아" 

# 실전에서도 반드시 위의 방법으로 중복되는 데이터들을 정리해야 합니다!


# 3.1.4 필요 없는 단어 제거하기 – 특정 단어나 글자수로 제거하기 ----------------------------------

# 주어진 문장을 단어로 잘라 내고 중복된 단어나 리스트를 제거 한 후 결과를 보면 
# 여전히 필요없어서 제거되어야 할 단어들이 있습니다. 위의 예에서는 “나” , “최고” 등의 글자들이죠?
# 이번에는 필요 없는 글자들을 어떻게 제거하는 지를 살펴 보겠습니다.

txt4
# [[1]]
# [1] "나"     "사과"   "바나나"
# 
# [[2]]
# [1] "나"     "바나나" "최고"  
# 
# [[3]]
# [1] "나"     "복숭아" "사과"  
# 
# [[4]]
# [1] "나"   "사과" "포도"
# 
# [[5]]
# [1] "나"       "파인애플" "복숭아"

# 먼저 특정 글자를 없애는 기능으로 gsub( ) 함수를 이용하겠습니다.
# 문법 : gsub(“변경전글자” , ”변경후글자” , data)

txt5 <- rapply(txt4, function(x) gsub("최고", "", x), how = "replace")
txt5
# [[1]]
# [1] "나"     "사과"   "바나나"
# 
# [[2]]
# [1] "나"     "바나나" ""      # <- 여기에 있던 “최고” 라는 글자 없어졌죠?
# 
# [[3]]
# [1] "나"     "복숭아" "사과"  
# 
# [[4]]
# [1] "나"   "사과" "포도"
# 
# [[5]]
# [1] "나"       "파인애플" "복숭아"  

# 만약 제거해야 할 글자가 많다면 위 코드를 여러 번 쓰는 것이 불편합니다.
# 그럴 경우 아래와 같이 제거하고 싶은 단어를 파일에 별도로 저장한 후 
# 반복문에서 불러와서 제거하도록 코드를 작성하면 됩니다.

txt <- readLines("gsub.txt")
txt
cnt_txt <- length(txt)
cnt_txt
for(i in 1:cnt_txt) {
  txt5 <- rapply(txt4, function(x) gsub((txt[i]), "", x), how = "replace")
}
txt

# 위 방법을 이용하면 특정 글자를 다른 것으로 변경할 수도 있습니다.
# 아래의 예를 보세요.

txt6 <- rapply(txt5, function(x) gsub("포도", "청포도", x), how = "replace")
txt6
# [[1]]
# [1] "나"     "사과"   "바나나"
# 
# [[2]]
# [1] "나"     "바나나" ""      
# 
# [[3]]
# [1] "나"     "복숭아" "사과"  
# 
# [[4]]
# [1] "나"     "사과"   "청포도"  #  포도가 청포도로 바꼈죠?
# 
# [[5]]
# [1] "나"       "파인애플" "복숭아"  

# 위 gusb( ) 함수에 정규식 을 사용할 수도 있습니다.
# 아래의 예를 보세요.

# data1 <- readLines("gsub연습_정규식.txt")
data1 <- c("101 홍길동 ?",
           "102 일지매 #",
           "103 유관순 .",
           "104 강감찬 /",
           "105 을지문덕 \\",
           "106 김유신 1000")
data1
# [1] "101 홍길동 ?"    "102 일지매 #"    "103 유관순 ."    "104 강감찬 /"    "105 을지문덕 \\" "106 김유신 1000"

data2 <- Map(extractNoun,data1)
data2
# $`101 홍길동 ?`
# [1] "101"    "홍길동"
# 
# $`102 일지매 #`
# [1] "102" "지"  "#"  
# 
# $`103 유관순 .`
# [1] "103"    "유관순"
# 
# $`104 강감찬 /`
# [1] "104"    "강감찬"
# 
# $`105 을지문덕 \\`
# [1] "105"      "을지문덕" "\\"      
# 
# $`106 김유신 1000`
# [1] "106"    "김유신" "100"    "0"     

# 위 결과를 보니 “일지매” 와 “유관순” 이 이상하게 출력된 것 보이죠?
# 사전 파일에 위 단어를 등록한 후 다시 불러 보겠습니다.

# mergeUserDic(data.frame(readLines("merge.txt"), "ncn"))
mergeUserDic(data.frame(c("일지매", "유관순"), c("ncn")))

# data1 <- readLines("gsub연습_정규식.txt")
# data1
data1 <- c("101 홍길동 ?",
           "102 일지매 #",
           "103 유관순 .",
           "104 강감찬 /",
           "105 을지문덕 \\",
           "106 김유신 1000")
data1

data2 <- Map(extractNoun,data1)
data2
# $`101 홍길동 ?`
# [1] "101"    "홍길동"
# 
# $`102 일지매 #`
# [1] "102"    "일지매" "#"     
# 
# $`103 유관순 .`
# [1] "103"    "유관순"
# 
# $`104 강감찬 /`
# [1] "104"    "강감찬"
# 
# $`105 을지문덕 \\`
# [1] "105"      "을지문덕" "\\"      
# 
# $`106 김유신 1000`
# [1] "106"    "김유신" "100"    "0" 

# 위 결과에서 모든 숫자를 한번에 제거해 보겠습니다.
# 정규식에서 숫자는 \\d 였던거 기억하시죠??

data3 <- rapply(data2, function(x) gsub("\\d+", "", x), how = "replace")
data3
# $`101 홍길동 ?`
# [1] ""       "홍길동"
# 
# $`102 일지매 #`
# [1] ""       "일지매" "#"     
# 
# $`103 유관순 .`
# [1] ""       "유관순"
# 
# $`104 강감찬 /`
# [1] ""       "강감찬"
# 
# $`105 을지문덕 \\`
# [1] ""         "을지문덕" "\\"      
# 
# $`106 김유신 1000`
# [1] ""       "김유신" ""       ""    

# 위 결과를 보면 모든 숫자가 다 제거 된 것이 확인됩니다.
# 여기서는 숫자만 제거했지만 다른 정규식도 마찬가지로 사용할 수 있습니다.
# 한가지 주의 사항은 메타 캐릭터는 \ (탈출문자)를 하나 더 쓰고 제거해야 합니다.
# 이번에는 글자수로 제거하는 방법을 보여드리겠습니다.
# 아래 예는 txt6 에 담겨있는 값 중에서 1글자 보다 크고 6글자 이하인 것만 남기고 모두 삭제하는 코드입니다.

txt7 <- sapply(txt6, function(x) {Filter(function(y) {nchar(y) <= 6 && nchar(y) > 1 },x)} )
txt7
# [[1]]
# [1] "사과"   "바나나"
# 
# [[2]]
# [1] "바나나"
# 
# [[3]]
# [1] "복숭아" "사과"  
# 
# [[4]]
# [1] "사과"   "청포도"
# 
# [[5]]
# [1] "파인애플" "복숭아" 

# 만약 아래와 같이 쓴다면 1글자 보다 크고 3글자 이하인 글자들만 남기라는 뜻이겠죠?

txt8 <- sapply(txt7, function(x) {Filter(function(y) {nchar(y) <= 3 && nchar(y) > 1 },x)} )
txt8
# [[1]]
# [1] "사과"   "바나나"
# 
# [[2]]
# [1] "바나나"
# 
# [[3]]
# [1] "복숭아" "사과"  
# 
# [[4]]
# [1] "사과"   "청포도"
# 
# [[5]]
# [1] "복숭아" #  여기 있던 파인애플이 삭제되었죠

# 여기까지 한글 텍스트 분석을 위한 패키지인 KoNLP( ) 패키지와 여러 가지 함수들을 사용하여 글자를 바꾸거나 제거하는 방법들을 살펴 보았습니다

# 3.1.5 extractNoun( ) 함수 사용시 UTF-8 에러 처리하기 -------------------------------

# 가끔 extractNoun 함수를 사용하여 문장을 단어로 분리할 때 아래와 같은 에러가 발생하는 경우가 있습니다.

tran1 <- Map(extractNoun, data1)
# Error in `Encoding<-`(`*tmp*`, value = "UTF-8") :
#   문자형 벡터 인자가 와야 합니다
# 추가정보: 50건 이상의 경고들을 발견되었습니다 (이들 중 처음 50건을 확인하기 위해서는…

## 위 에러는 띄어쓰기가 없이 너무 긴 문장이 들어갈 경우 발생하는 에러입니다.
## 아래의 nchar 뒤에 들어가는 글자수는 작업할 때 마다 적절하게 조절해서 사용하면 됩니다

data11 <- Filter(function(x) { nchar(x) <= 200 } , data1)
data11

tran1 <- Map(extractNoun, data11)
tran1

# 이하 생략 - 자주 나오는 에러 이므로 이 방법을 꼭 기억해 두세요 ~


# 3.2 wordcloud( ) 패키지 배우기 ------------------------------------------------

# 이 패키지는 Ian Fellows 님이 워드 클라우드를 만들기 위해 제작한 패키지 입니다.
# 이 패키지 역시 다양한 함수를 가지고 있지만 이번 실습에서 사용된 wordcloud 함수만 살펴보겠습니다. 
#  이 패키지에 대한 전체 설명을 보시고 싶다면 구글에서 " wordcloud in r " 로 검색하면 자세한 정보가 나옵니다.

# 문법 :
# wordcloud(words,freq,scale=c(4,.5),min.freq=3,max.words=Inf,random.order=TRUE,
#           random.color=FALSE,rot.per=.1,colors="black",ordered.colors=FALSE,
#           use.r.layout=FALSE,fixed.asp=TRUE, ...)

# 주요 옵션 설명 :
# - words : 출력할 단어들
# - freq : 언급된 빈도수
# - scale : 글자크기
# - min.freq : 최소언급횟수지정 - 이 값 이상 언급된 단어만 출력합니다.
# - max.words : 최대언급횟수지정. 이 값 이상 언급되면 삭제됩니다.
# - random.order : 출력되는 순서를 임의로 지정합니다
# - random.color : 글자 색상을 임의로 지정합니다.
# - rot.per : 단어배치를 90 도 각도로 출력합니다.
# - colors : 출력될 단어들의 색상을 지정합니다.
# - ordered.colors : 이 값을 true 로 지정할 경우 각 글자별로 색상을 순서대로 지정할 수 있습니다.
# - use.r.layout : 이 값을 false 로 할 경우 R 에서 c++ 코드를 사용할 수 있습니다.

# install.packages("wordcloud2")
library(wordcloud2)
example("wordcloud2")

wordcloud2(demoFreq, color = ifelse(demoFreq[, 2] > 20, 'red', 'skyblue'))
# shape = “diamond” , “star” , “circle” 등 다양한 옵션 사용 가능함.

