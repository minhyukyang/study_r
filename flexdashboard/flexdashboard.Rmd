---
title: "AIM x LAB"
runtime : shiny
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout : scroll
    theme: yeti
---

```{r global_packages}
# library(devtools)
# devtools::install_github("lovetoken/rdevteam")
# https://github.com/lovetoken/rdevteam
# https://lovetoken.gitbooks.io/r-dev-team-open-seminar/content/b3c4-c804-acfc-c81c/b124-c774-bc84-c5f0-ad00-ac80-c0c9-c5b4-cd94-cd9c-c800-c7a5-d504-b85c-adf8-b7a8-b9cc-b4e4-ae30/ad00-b828-ae30-c220-bcc4-b9ac-c11c-ce58.html
library(rdevteam)
library(shiny)
library(flexdashboard)
library(dplyr)
library(DT)
library(networkD3)
library(bsplus)
library(KoNLP)
library(tesseract)
library(magick)
library(ggplot2)
library(knitr)
library(tidytext)
library(tidyverse)
library(SnowballC)
# setwd("U:/Analysis/R/study_r/flexdashboard/")
setwd("d:/Analysis/R/study_r/flexdashboard/")
```
Sidebar {.sidebar}
=================================================

```{r sidebar_ui}
textInput("ui1", "Keyword", "챗봇")
actionButton("act", "Change apply")

d <- reactive({
  keyword <- input$ui1
  naverRelation(keyword, depth=2)
})
```

Summary
=================================================

### Plot

```{r}
renderForceNetwork({
  req(input$act)
  isolate({
    d() %>% plot
  })
})
```

### Rawdata

```{r}
renderDT({
  req(input$act)
  isolate({
    d() %>% datatable
  })
})
```

OCR {data-vertical_layout=scroll}
=================================================

OCR Demo

column
-------------------------------------------------

### Image
```{r, fig.retina = 1}
knitr::include_graphics("pic/receipt.png") 

actionButton("ocr_act", "Analysis")

# ocr_proc <- reactive({
#   image_read("pic/receipt.png")
# })
```

column
-------------------------------------------------

### Extract
```{r}
renderPrint({
  req(input$ocr_act)
  isolate({
    image <- image_read("pic/receipt.png")
    image_bearb <- image %>%
      image_scale("x2000") %>%                        # rescale
      image_background("white", flatten = TRUE) %>%   # set background to white
      image_trim() %>%                                # Trim edges that are the background color from the image.
      image_noise() %>%                               # Reduce noise in image using a noise peak elimination filter
      image_enhance() %>%                             # Enhance image (minimize noise)
      image_normalize() %>%                           # Normalize image (increase contrast by normalizing the pixel values to span the full range of color values).
      image_contrast(sharpen = 1)    
    
    whitelist <- "1234567890-.,;:qwertzuiopüasdfghjklöäyxcvbnmQWERTZUIOPÜASDFGHJKLÖÄYXCVBNM@ß€!$%&/()=?+"
    text <- ocr(image_bearb, engine = tesseract(language = "eng", options = list(tessedit_char_whitelist = whitelist)))
    text_df <- data.frame(text = read.delim(textConnection(text),    # make text into dataframe
                                            header = FALSE, 
                                            sep = "\n", 
                                            strip.white = TRUE))
    text_df %>% datatable
  })
})
# 
# image_bearb <- image %>%
#   image_scale("x2000") %>%                        # rescale
#   image_background("white", flatten = TRUE) %>%   # set background to white
#   image_trim() %>%                                # Trim edges that are the background color from the image.
#   image_noise() %>%                               # Reduce noise in image using a noise peak elimination filter
#   image_enhance() %>%                             # Enhance image (minimize noise)
#   image_normalize() %>%                           # Normalize image (increase contrast by normalizing the pixel values to span the full range of color values).
#   image_contrast(sharpen = 1)                  # increase contrast
#   # image_deskew(treshold = 40)                     # deskew image -> creates negative offset in some scans
# 
# # image_browse(image_bearb)
# # image_write(image_bearb, path = "flexdashboard/pic/receipt_scan_bearb.png", format = "png")
# 
# whitelist <- "1234567890-.,;:qwertzuiopüasdfghjklöäyxcvbnmQWERTZUIOPÜASDFGHJKLÖÄYXCVBNM@ß€!$%&/()=?+"
# 
# text <- ocr(image_bearb,
# # text <- ocr("flexdashboard/pic/receipt_scan_bearb.png",
#             engine = tesseract(language = "eng", 
#                                options = list(tessedit_char_whitelist = whitelist)))
# 
# text_df <- data.frame(text = read.delim(textConnection(text),    # make text into dataframe
#                                         header = FALSE, 
#                                         sep = "\n", 
#                                         strip.white = TRUE))
# text_df <- data.frame(text = read.delim(textConnection(text),    # make text into dataframe
#                                         header = FALSE, 
#                                         sep = "\n", 
#                                         strip.white = TRUE)) %>%
#   mutate(text = as.character(V1)) %>%
# #   unnest_tokens(word, text) %>%                                      # separate words
# #   mutate(word = wordStem(word, language = "eng"))                    # use word stem
# 
# text_df %>% datatable

```

Sentiment Analysis
=================================================

Sentiment Analysis Demo

### Input

```{r}
textAreaInput("senti_anal_ui", "sentence", 
          width = "400px", height = "100%", rows = "6", resize = "both",
          value = "현대모비스 빅데이터팀의 팀명이 AIM LAB으로 변경되었습니다.")

actionButton("senti_anal_act", "Analysis")

sa_proc_noun <- reactive({
  sentence <- input$senti_anal_ui
  print("# extractNoun")
  KoNLP::extractNoun(sentence)
})

sa_proc_automata <- reactive({
  sentence <- input$senti_anal_ui
  print("# HangulAutomata")
  KoNLP::HangulAutomata(sentence)
})

# sa_proc_morph <- reactive({
#   sentence <- input$senti_anal_ui
#   print("# MorphAnalyzer")
#   KoNLP::MorphAnalyzer(sentence)
# })
#
# 
# print("HangulAutomata")
#   KoNLP::HangulAutomata(sentence)
#   print("SimplePos09")
#   KoNLP::SimplePos09(sentence)
#   print("SimplePos22")
#   KoNLP::SimplePos22(sentence)
```

### Output

```{r}
renderPrint({
  req(input$senti_anal_act)
  isolate({
    sa_proc_noun() %>% print
  })
  isolate({
    sa_proc_automata() %>% print
  })
})
```


Text Summarization
=================================================

Textrank 알고리즘을 이용하여 문서를 n개의 문장으로 요약하는 예제입니다.

### Input

```{r input_text}
# textInput("ui2", "Sentence", 
#           width = "100%", 
#           value = "현대모비스 빅데이터팀의 팀명이 AIM LAB으로 변경되었습니다.")
textAreaInput("ui2", "Sentence", 
          width = "100%", height = "100%", rows = "6", resize = "both",
          value = "현대모비스 빅데이터팀의 팀명이 AIM LAB으로 변경되었습니다.")

actionButton("act2", "Analysis")

d2 <- reactive({
  sentence <- input$ui2
  extractNoun(sentence)
})
```

### Output

```{r}
renderPrint({
  req(input$act2)
  isolate({
    d2() %>% print
  })
})
```

QA
=================================================

MRC(BERT?)


About
=================================================

본 대쉬보드는 네이버 연관검색어 서비스를 기반으로 합니다.
사용된 도구는 R, Shiny package, flexdashboard packages 등이 이용됩니다.

# 참고
---
flexdashboard 소개
https://m.blog.naver.com/PostView.nhn?blogId=hsj2864&logNo=221093569545&proxyReferer=https%3A%2F%2Fwww.google.com%2F
